{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import webbrowser\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = pd.read_excel(\"Projects_v3.xlsx\", sheet_name=\"VCM BC projects\")\n",
    "# i don't concat directly because I wanted to harmonize the column names\n",
    "sheets = {'car':'CAR Projects','vcs': 'VCS Projects', 'acr': 'ACR Projects','gold': 'Gold Projects'}\n",
    "berkeley = {}\n",
    "for v, k in sheets.items():\n",
    "    berkeley[v] = pd.read_excel(\"Voluntary-Registry-Offsets-Database--v8-May-2023.xlsx\", sheet_name=k).dropna(how='all', axis=1)\n",
    "\n",
    "berkeley['vcs'] = berkeley['vcs'].drop(['Project ID'], axis=1)\n",
    "berkeley['vcs'] = berkeley['vcs'].rename(columns={\"Registry ID\": \"Project ID\"})\n",
    "berkeley['gold'] = berkeley['gold'].rename(columns={\"GS_ID\": \"Project ID\"})\n",
    "berkeleyDS = pd.concat(berkeley.values())\n",
    "berkeleyDS.dropna(subset=['Project ID'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will find nonsense values for non-registry ones, but we filter later\n",
    "projects[\"registryID\"] = (\n",
    "    projects[\"Registry entry\"].apply(\n",
    "        lambda x: re.findall(r\"-?\\d+\\.?\\d*\", x)[-1] if type(x) == str else x\n",
    "    )\n",
    ").astype(\"Int64\")\n",
    "\n",
    "\n",
    "berkeleyDS['plainID'] = (\n",
    "    berkeleyDS[\"Project ID\"].apply(\n",
    "        lambda x: re.findall(r\"-?\\d+\\.?\\d*\", x)[-1] if type(x) == str else x\n",
    "    )\n",
    ").astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['registryID'] = projects['registryID'].astype(str)\n",
    "projects.loc[projects['Registry entry'].str.contains(r'thereserve2', na=False), 'projectID'] = 'CAR' + projects['registryID'] \n",
    "projects.loc[projects['Registry entry'].str.contains(r'verra', na=False), 'projectID'] = 'VCS' + projects['registryID'] \n",
    "# drop duplicated columns as we merge\n",
    "projectsM = berkeleyDS.merge(projects, left_on=\"Project ID\",right_on=\"projectID\", how=\"outer\", suffixes=('', '_y'))\n",
    "projectsM.dropna(how='all', axis=1, inplace=True)\n",
    "projectsM.drop(projectsM.filter(regex='_y$').columns, axis=1, inplace=True)\n",
    "projectsM.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mangrove projects not in list\n",
    "mask = projectsM['Project Name'].str.contains(r'mangrove', na=False, case=False)\n",
    "projectsM.loc[mask, 'projectID'] = projectsM['Project ID'].astype(str)\n",
    "projectsM.loc[mask, 'registryID'] = projectsM['Project ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you start from Berkeley database\n",
    "# missing ACR and gold \n",
    "projectsM.plainID = projectsM.plainID.astype(str)\n",
    "projectsM.loc[projectsM['Project ID'].str.contains(r'CAR', na=False), 'projectLink'] = 'https://thereserve2.apx.com/mymodule/reg/TabDocuments.asp?r=111&ad=Prpt&act=update&type=PRO&aProj=pub&tablename=doc&id1=' + projectsM['plainID'] \n",
    "projectsM.loc[projectsM['Project ID'].str.contains(r'VCS', na=False), 'projectLink'] = 'https://registry.verra.org/uiapi/resource/resourceSummary/' + projectsM['plainID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of projects \n",
    "projectsW = projectsM[~projectsM['registryID'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 43.4 ms, total: 1.36 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the links to the geospatial data file in the registry, only VCS and CAR for now\n",
    "projectsW['urlDown'] = ''\n",
    "projectsW['urlFileNames'] = ''\n",
    "for url in projectsW.projectLink:\n",
    "    if type(url) is str:\n",
    "        if 'verra' in url:\n",
    "            data = requests.get(url).json()\n",
    "            # we get the lon,lat as in the JSON file\n",
    "            projectsW.loc[projectsW['projectLink'] == url, 'latitude'] = data['location']['latitude']\n",
    "            projectsW.loc[projectsW['projectLink'] == url, 'longitude'] = data['location']['longitude']\n",
    "            # we also get the KML, if any, to compute the coordinates manually\n",
    "            # the nesting in the JSON file is nasty\n",
    "            found_match = False\n",
    "            for group in data['documentGroups']:\n",
    "                if 'OTHER_DOCUMENTS' in next(iter(group.values())):\n",
    "                    for docs in group['documents']:\n",
    "                        regexp = re.compile(r'\\.kml',re.IGNORECASE)\n",
    "                        if regexp.search(docs['documentName']):\n",
    "                            projectsW.loc[projectsW['projectLink'] == url, 'urlDown'] = docs['uri']\n",
    "                            projectsW.loc[projectsW['projectLink'] == url, 'urlFileNames'] = docs['documentName']\n",
    "                            found_match = True\n",
    "                            break  # Stop looking for more matches\n",
    "                if found_match:\n",
    "                    break  # Stop iterating over 'documentGroups'\n",
    "        elif 'thereserve2' in url:\n",
    "            reqs = requests.get(url)\n",
    "            soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True, string=re.compile(r'shapefile|shape file|shape|shp|kml', re.IGNORECASE))\n",
    "            if len(links)>0:\n",
    "                projectsW.loc[projectsW['projectLink'] == url, 'urlDown'] = 'https://thereserve2.apx.com' + links[0].get('href')\n",
    "                projectsW.loc[projectsW['projectLink'] == url, 'urlFileNames'] = links[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what files have been downloaded already \n",
    "projectsW.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "downloaded = [\n",
    "    os.path.splitext(filename)[0] for filename in os.listdir(\"./locationFiles/\")\n",
    "]\n",
    "downList = []\n",
    "for project in list(set(projectsW['projectID'])):\n",
    "        if project not in downloaded:\n",
    "                downList.append(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file and name it with as the project ID. \n",
    "for url in projectsW.urlDown:\n",
    "      if type(url) is str:\n",
    "        if projectsW['projectID'][projectsW['urlDown'] == url].astype(str).values[0] in downList:\n",
    "            try:\n",
    "                 downFile = requests.get(url)\n",
    "                 fileName = './locationFiles/{}'.format(projectsW['projectID'][projectsW['urlDown'] == url].astype(str).values[0])\n",
    "                 ext = re.findall('(\\.[^.]*)$',projectsW['urlFileNames'][projectsW['urlDown'] == url].astype(str).values[0])[-1]\n",
    "                 if ext != 'zip':  \n",
    "                      with open(\"%s%s\" % (fileName, ext), 'wb') as f:\n",
    "                                for chunk in downFile.iter_content(1024): # iterate on stream using 1KB packets\n",
    "                                    f.write(chunk) # write the file\n",
    "                 else:\n",
    "                      with open(fileName, 'wb') as f:\n",
    "                                for chunk in downFile.iter_content(1024): # iterate on stream using 1KB packets\n",
    "                                    f.write(chunk) # write the file               \n",
    "            except:\n",
    "                 pass\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectsW.dropna(how='all', axis=1, inplace=True)\n",
    "projectsW.to_csv('projectsW.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectsW = pd.read_csv('projectsW.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To test\n",
    "As of now, no fix for zip files. I need to rename folder and files in zip keeping the extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_zip_path = './locationFiles/CAR1429.zip'\n",
    "target_zip_path = './locationFiles/CAR1429test.zip'\n",
    "\n",
    "# Extract the zip file to a temporary directory\n",
    "temp_dir = './locationFiles/temp'\n",
    "with zipfile.ZipFile(source_zip_path, 'r') as source_zip:\n",
    "    source_zip.extractall(temp_dir)\n",
    "\n",
    "# Get the folder name inside the extracted files\n",
    "folder_name = os.path.basename(temp_dir)\n",
    "\n",
    "# Create a new target zip file\n",
    "with zipfile.ZipFile(target_zip_path, 'w') as target_zip:\n",
    "    # Iterate over the extracted files\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(file_path, temp_dir)\n",
    "            \n",
    "            # Get the extension of the file\n",
    "            file_extension = os.path.splitext(file)[1]\n",
    "            \n",
    "            # Construct the new file name using the zip file name and the folder name\n",
    "            new_file_name = f\"{folder_name}{file_extension}\"\n",
    "            \n",
    "            # Write the file to the target zip file with the new name and relative path\n",
    "            target_zip.write(file_path, arcname=os.path.join(folder_name, new_file_name))\n",
    "\n",
    "# Remove the temporary directory\n",
    "os.rmdir(temp_dir)\n",
    "\n",
    "print(\"Zip file renamed and files inside renamed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
