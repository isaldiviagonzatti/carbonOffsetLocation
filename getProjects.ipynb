{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import webbrowser\n",
    "import time\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = pd.read_excel(\"Projects_v3.xlsx\", sheet_name=\"VCM BC projects\")\n",
    "# i don't concat directly because I wanted to harmonize the column names\n",
    "sheets = {'car':'CAR Projects','vcs': 'VCS Projects', 'acr': 'ACR Projects','gold': 'Gold Projects'}\n",
    "berkeley = {}\n",
    "for v, k in sheets.items():\n",
    "    berkeley[v] = pd.read_excel(\"Voluntary-Registry-Offsets-Database--v8-May-2023.xlsx\", sheet_name=k).dropna(how='all', axis=1)\n",
    "\n",
    "berkeley['vcs'] = berkeley['vcs'].drop(['Project ID'], axis=1)\n",
    "berkeley['vcs'] = berkeley['vcs'].rename(columns={\"Registry ID\": \"Project ID\"})\n",
    "berkeley['gold'] = berkeley['gold'].rename(columns={\"GS_ID\": \"Project ID\"})\n",
    "berkeleyDS = pd.concat(berkeley.values())\n",
    "berkeleyDS.dropna(subset=['Project ID'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only VCS has mangrove projects in the project names\n",
    "additional = berkeley['vcs'][berkeley['vcs']['Project Name'].str.contains(r'mangrove', na=False, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['registryID'] = projects['registryID'].astype(str)\n",
    "projects.loc[projects['Registry entry'].str.contains(r'thereserve2', na=False), 'projectID'] = 'CAR' + projects['registryID'] \n",
    "projects.loc[projects['Registry entry'].str.contains(r'verra', na=False), 'projectID'] = 'VCS' + projects['registryID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated columns as we merge\n",
    "projectsM = projects.merge(berkeleyDS, left_on=\"projectID\",right_on=\"Project ID\", how=\"left\", suffixes=('', '_y'))\n",
    "projectsM.dropna(how='all', axis=1, inplace=True)\n",
    "projectsM.drop(projectsM.filter(regex='_y$').columns, axis=1, inplace=True)\n",
    "projectsM.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant, but in case you use for the Berkeley dataset\n",
    "projectsM.loc[projectsM['projectID'].str.contains(r'CAR', na=False), 'projectLink'] = 'https://thereserve2.apx.com/mymodule/reg/TabDocuments.asp?r=111&ad=Prpt&act=update&type=PRO&aProj=pub&tablename=doc&id1=' + projectsM['registryID'] \n",
    "projectsM.loc[projectsM['projectID'].str.contains(r'VCS', na=False), 'projectLink'] = 'https://registry.verra.org/uiapi/resource/resourceSummary/' + projectsM['registryID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 ms, sys: 7.92 ms, total: 36.9 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "projectsM['urlDown'] = ''\n",
    "projectsM['urlFileNames'] = ''\n",
    "for url in ['https://registry.verra.org/uiapi/resource/resourceSummary/1463']:\n",
    "    if type(url) is str:\n",
    "        # VCS and CAR for now\n",
    "        if 'verra' in url:\n",
    "            data = requests.get(url).json()\n",
    "            # we get the lon,lat as in the JSON file\n",
    "            projectsM.loc[projectsM['projectLink'] == url, 'latitude'] = data['location']['latitude']\n",
    "            projectsM.loc[projectsM['projectLink'] == url, 'longitude'] = data['location']['longitude']\n",
    "            # we also get the KML, if any, to compute the coordinates manually\n",
    "            # the nesting in the JSON file is nasty\n",
    "            found_match = False\n",
    "            for group in data['documentGroups']:\n",
    "                if 'OTHER_DOCUMENTS' in next(iter(group.values())):\n",
    "                    for docs in group['documents']:\n",
    "                        regexp = re.compile(r'\\.kml',re.IGNORECASE)\n",
    "                        if regexp.search(docs['documentName']):\n",
    "                            projectsM.loc[projectsM['projectLink'] == url, 'urlDown'] = docs['uri']\n",
    "                            projectsM.loc[projectsM['projectLink'] == url, 'urlFileNames'] = docs['documentName']\n",
    "                            found_match = True\n",
    "                            break  # Stop looking for more matches\n",
    "                if found_match:\n",
    "                    break  # Stop iterating over 'documentGroups'\n",
    "        elif 'thereserve2' in url:\n",
    "            reqs = requests.get(url)\n",
    "            soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True, string=re.compile(r'shapefile|shape file|shape|shp|kml', re.IGNORECASE))\n",
    "            if len(links)>0:\n",
    "                projectsM.loc[projectsM['projectLink'] == url, 'urlDown'] = 'https://thereserve2.apx.com' + links[0].get('href')\n",
    "                projectsM.loc[projectsM['projectLink'] == url, 'urlFileNames'] = links[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what files have been downloaded already \n",
    "projectsM.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "downloaded = [\n",
    "    os.path.splitext(filename)[0] for filename in os.listdir(\"./locationFiles/\")\n",
    "]\n",
    "downList = []\n",
    "for project in list(set(projectsM['projectID'])):\n",
    "        if project not in downloaded:\n",
    "                downList.append(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file and name it with as the project ID. \n",
    "for url in projectsM.urlDown:\n",
    "      if type(url) is str:\n",
    "        if projectsM['projectID'][projectsM['urlDown'] == url].astype(str).values[0] in downList:\n",
    "            try:\n",
    "                 downFile = requests.get(url)\n",
    "                 fileName = './locationFiles/{}'.format(projectsM['projectID'][projectsM['urlDown'] == url].astype(str).values[0])\n",
    "                 ext = re.findall('(\\.[^.]*)$',projectsM['urlFileNames'][projectsM['urlDown'] == url].astype(str).values[0])[-1]\n",
    "                 if ext != 'zip':  \n",
    "                      with open(\"%s%s\" % (fileName, ext), 'wb') as f:\n",
    "                                for chunk in downFile.iter_content(1024): # iterate on stream using 1KB packets\n",
    "                                    f.write(chunk) # write the file\n",
    "                 else:\n",
    "                      with open(fileName, 'wb') as f:\n",
    "                                for chunk in downFile.iter_content(1024): # iterate on stream using 1KB packets\n",
    "                                    f.write(chunk) # write the file               \n",
    "            #   open('./locationFiles/{}'.format(projectsM['projectID'][projectsM['urlDown'] == url].astype(str).values[0]), 'wb').write(downFile.content)\n",
    "            except:\n",
    "                 pass\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will find nonsense values for non-registry ones, but we filter later\n",
    "# projects[\"registryID\"] = (\n",
    "#     projects[\"Registry entry\"].apply(\n",
    "#         lambda x: re.findall(r\"-?\\d+\\.?\\d*\", x)[-1] if type(x) == str else x\n",
    "#     )\n",
    "# ).astype(\"Int64\")\n",
    "\n",
    "# berkeleyDS['registryID'] = (\n",
    "#     berkeleyDS[\"Project ID\"].apply(\n",
    "#         lambda x: re.findall(r\"-?\\d+\\.?\\d*\", x)[-1] if type(x) == str else x\n",
    "#     )\n",
    "# ).astype(\"Int64\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
